{
    "cells": [
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# Regression data using scikit-learn\n\nRegression is when the feature to be predicted contains continuous values. Regression refers to the process of predicting a dependent variable by analyzing the relationship between other independent variables. There are several algorithms known to us that help us in excavating these relationships to better predict the value.\n\nIn this notebook, we'll use scikit-learn to predict values. Scikit-learn provides implementations of many regression algorithms. In here, we have done a comparative study of 5 different regression algorithms. \n\nTo help visualize what we are doing, we'll use 2D and 3D charts to show how the classes looks (with 3 selected dimensions) with matplotlib and seaborn python libraries.\n\n\n<a id=\"top\"></a>\n## Table of Contents\n\n1. [Load libraries](#load_libraries)\n2. [Helper methods for metrics](#helper_methods)\n3. [Data exploration](#explore_data)\n4. [Prepare data for building regression model](#prepare_data)\n5. [Build Simple Linear Regression model](#model_slr)\n6. [Build Multiple Linear Regression classification model](#model_mlr)\n7. [Build Polynomial Linear Regression model](#model_plr) \n8. [Build Decision Tree Regression model](#model_dtr) \n9. [Build Random Forest Regression model](#model_rfr)\n10. [Comparitive study of different regression algorithms](#compare_classification)"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Quick set of instructions to work through the notebook\n\nIf you are new to Notebooks, here's a quick overview of how to work in this environment.\n\n1. The notebook has 2 types of cells - markdown (text) such as this and code such as the one below. \n2. Each cell with code can be executed independently or together (see options under the Cell menu). When working in this notebook, we will be running one cell at a time because we need to make code changes to some of the cells.\n3. To run the cell, position cursor in the code cell and click the Run (arrow) icon. The cell is running when you see the * next to it. Some cells have printable output.\n4. Work through this notebook by reading the instructions and executing code cell by cell. Some cells will require modifications before you run them. "
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "<a id=\"load_libraries\"></a>\n## 1. Load libraries\n[Top](#top)\n\nInstall python modules\nNOTE! Some pip installs require a kernel restart.\nThe shell command pip install is used to install Python modules. Some installs require a kernel restart to complete. To avoid confusing errors, run the following cell once and then use the Kernel menu to restart the kernel before proceeding."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "!pip install pandas==0.24.2\n!pip install --user pandas_ml==0.6.1\n#downgrade matplotlib to bypass issue with confusion matrix being chopped out\n!pip install matplotlib==3.1.0\n!pip install seaborn\n!pip install pydot\n!pip install graphviz\n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer, make_column_transformer\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,mean_squared_error, r2_score\n\nimport pandas as pd, numpy as np\nimport sys\nimport io\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport seaborn as sns\nfrom sklearn.tree import export_graphviz\nimport pydot\n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "\n<a id=\"helper_methods\"></a>\n## 2. Helper methods for metrics\n[Top](#top)\n\n"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "\ndef two_d_compare(X_test,y_test,y_pred,model_name):\n    area = (12 * np.random.rand(40))**2 \n    plt.subplots(ncols=2, figsize=(10,4))\n    plt.suptitle('Actual vs Predicted data : ' +model_name + '. Variance score: %.2f' % r2_score(y_test, y_pred))\n\n    plt.subplot(121)\n    plt.scatter(X_test, y_test, alpha=0.8, color='#8CCB9B')\n    plt.title('Actual')\n\n    plt.subplot(122)\n    plt.scatter(X_test, y_pred,alpha=0.8, color='#E5E88B')\n    plt.title('Predicted')\n\n    plt.show()\n    \n\ndef model_metrics(regressor,y_test,y_pred):\n    mse = mean_squared_error(y_test,y_pred)\n    print(\"Mean squared error: %.2f\"\n      % mse)\n    r2 = r2_score(y_test, y_pred)\n    print('R2 score: %.2f' % r2 )\n    return [mse, r2]\n\ndef two_vs_three(x_test,y_test,y_pred,z=None, isLinear = False) : \n    \n    area = 60\n    \n\n    fig = plt.figure(figsize=(12,6))\n    fig.suptitle('2D and 3D view of sales price data')\n\n    # First subplot\n    ax = fig.add_subplot(1, 2,1)\n    ax.scatter(x_test, y_test, alpha=0.5,color='blue', s= area)\n    ax.plot(x_test, y_pred, alpha=0.9,color='red', linewidth=2)\n    ax.set_xlabel('YEAR BUILT')\n    ax.set_ylabel('SELLING PRICE')\n    \n    plt.title('YEARBUILT vs SALEPRICE')\n    \n    if not isLinear : \n    # Second subplot\n        ax = fig.add_subplot(1,2,2, projection='3d')\n\n        ax.scatter(z, x_test, y_test, color='blue', marker='o')\n        ax.plot(z, x_test, y_pred, alpha=0.9,color='red', linewidth=2)\n        ax.set_ylabel('YEAR BUILT')\n        ax.set_zlabel('SELLING PRICE')\n        ax.set_xlabel('LOT AREA')\n\n    plt.title('LOT AREA vs YEAR BUILT vs SELLING PRICE')\n\n    plt.show()\n    ",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "<a id=\"explore_data\"></a>\n## 3. Data exploration\n[Top](#top)\n\nData can be easily loaded within IBM Watson Studio. Instructions to load data within IBM Watson Studio can be found [here](https://ibmdev1.rtp.raleigh.ibm.com/tutorials/watson-studio-using-jupyter-notebook/). The data set can be located by its name and inserted into the notebook as a pandas DataFrame as shown below.\n\n![insert_spark_dataframe.png](https://raw.githubusercontent.com/IBM/icp4d-customer-churn-classifier/master/doc/source/images/insert_spark_dataframe.png)\n\nThe generated code comes up with a generic name and it is good practice to rename the dataframe to match the use case context.\n"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "In the snippet below, we use the pandas library to load a csv that contains housing related information. With several independent variables related to this domain, we are going to predict the sales price of a house. "
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "df_pd =  pd.read_csv(\"https://raw.githubusercontent.com/IBM/ml-learning-path-assets/master/data/predict_home_value.csv\")\ndf_pd.head()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "area = 60\nx = df_pd['YEARBUILT']\ny = df_pd['SALEPRICE']\nz = df_pd['LOTAREA']\n\n\nfig = plt.figure(figsize=(12,6))\nfig.suptitle('2D and 3D view of sales price data')\n\n# First subplot\nax = fig.add_subplot(1, 2,1)\n\nax.scatter(x, y, alpha=0.5,color='blue', s= area)\nax.set_xlabel('YEAR BUILT')\nax.set_ylabel('SELLING PRICE')\n\nplt.title('YEARBUILT vs SALEPRICE')\n\n# Second subplot\nax = fig.add_subplot(1,2,2, projection='3d')\n\nax.scatter(z, x, y, color='blue', marker='o')\n\nax.set_ylabel('YEAR BUILT')\nax.set_zlabel('SELLING PRICE')\nax.set_xlabel('LOT AREA')\n\nplt.title('LOT AREA VS YEAR BUILT vs SELLING PRICE')\n\nplt.show()\n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "sns.set(rc={\"figure.figsize\": (8, 4)}); np.random.seed(0)\nax = sns.distplot(df_pd['SALEPRICE'])\nplt.show()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "\nprint(\"The dataset contains columns of the following data types : \\n\" +str(df_pd.dtypes))\n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Notice below that FIREPLACEQU, GARAGETYPE, GARAGEFINISH, GARAGECOND,FENCE and POOLQC have missing values. "
        },
        {
            "metadata": {
                "scrolled": true
            },
            "cell_type": "code",
            "source": "print(\"The dataset contains following number of records for each of the columns : \\n\" +str(df_pd.count()))\n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "df_pd.isnull().any()\n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "<a id=\"prepare_data\"></a>\n## 4. Data preparation\n[Top](#top)\n\nData preparation is a very important step in machine learning model building. This is because the model can perform well only when the data it is trained on is good and well prepared. Hence, this step consumes bulk of data scientist's time spent building models.\n\nDuring this process, we identify categorical columns in the dataset. Categories needed to be indexed, which means the string labels are converted to label indices. These label indices are encoded using One-hot encoding to a binary vector with at most a single one-value indicating the presence of a specific feature value from among the set of all feature values. This encoding allows algorithms which expect continuous features to use categorical features.\n"
        },
        {
            "metadata": {
                "scrolled": false
            },
            "cell_type": "code",
            "source": "\n#remove columns that are not required\ndf_pd = df_pd.drop(['ID'], axis=1)\n\ndf_pd.head()\n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Defining the categorical columns \ncategoricalColumns = df_pd.select_dtypes(include=[np.object]).columns\n\nprint(\"Categorical columns : \" )\nprint(categoricalColumns)\n\nimpute_categorical = SimpleImputer(strategy=\"most_frequent\")\nonehot_categorical =  OneHotEncoder(handle_unknown='ignore')\n\ncategorical_transformer = Pipeline(steps=[('impute',impute_categorical),('onehot',onehot_categorical)])",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Defining the numerical columns \nnumericalColumns = [col for col in df_pd.select_dtypes(include=[np.float,np.int]).columns if col not in ['SALEPRICE']]\nprint(\"Numerical columns : \" )\nprint(numericalColumns)\n\nscaler_numerical = StandardScaler()\n\nnumerical_transformer = Pipeline(steps=[('scale',scaler_numerical)])\n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "preprocessorForCategoricalColumns = ColumnTransformer(transformers=[('cat', categorical_transformer, categoricalColumns)],\n                                            remainder=\"passthrough\")\npreprocessorForAllColumns = ColumnTransformer(transformers=[('cat', categorical_transformer, categoricalColumns),('num',numerical_transformer,numericalColumns)],\n                                            remainder=\"passthrough\")\n\n\n#. The transformation happens in the pipeline. Temporarily done here to show what intermediate value looks like\ndf_pd_temp = preprocessorForCategoricalColumns.fit_transform(df_pd)\nprint(\"Data after transforming :\")\nprint(df_pd_temp)\n\ndf_pd_temp_2 = preprocessorForAllColumns.fit_transform(df_pd)\nprint(\"Data after transforming :\")\nprint(df_pd_temp_2)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {
                "scrolled": true
            },
            "cell_type": "code",
            "source": "# prepare data frame for splitting data into train and test datasets\n\nfeatures = []\nfeatures = df_pd.drop(['SALEPRICE'], axis=1)\n\nlabel = pd.DataFrame(df_pd, columns = ['SALEPRICE']) \n#label_encoder = LabelEncoder()\nlabel = df_pd['SALEPRICE']\n\n#label = label_encoder.fit_transform(label)\nprint(\" value of label : \" + str(label))\n\n\n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "<a id=\"model_slr\"></a>\n## 5. Simple linear regression\n[Top](#top)\n\nThis is the most basic form of linear regression in which the variable to be predicted is dependent on only one other variable. This is calculated by using the formula that is generally used in calculating the slope of a line.\n\ny = w0 + w1*x1\n\nIn the above equation, y refers to the target variable and x1 refers to the independent variable. w1 refers to the coeeficient that expresses the relationship between y and x1 is it also know as the slope. w0 is the constant cooefficient a.k.a the intercept. It refers to the constant offset that y will always be with respect to the independent variables."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Since simple linear regression assumes that output depends on only one variable, we are assuming that it depends on the YEARBUILT. Data is split up into training and test sets. "
        },
        {
            "metadata": {
                "scrolled": true
            },
            "cell_type": "code",
            "source": "X = features['YEARBUILT'].values.reshape(-1,1)\nX_train_slr, X_test_slr, y_train_slr, y_test_slr = train_test_split(X,label , random_state=0)\n\nprint(\"Dimensions of datasets that will be used for training : Input features\"+str(X_train_slr.shape)+ \n      \" Output label\" + str(y_train_slr.shape))\nprint(\"Dimensions of datasets that will be used for testing : Input features\"+str(X_test_slr.shape)+ \n      \" Output label\" + str(y_test_slr.shape))",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {
                "scrolled": true
            },
            "cell_type": "code",
            "source": "from sklearn.linear_model import LinearRegression\n\nmodel_name = 'Simple Linear Regression'\n\nslRegressor = LinearRegression()\n\nslRegressor.fit(X_train_slr,y_train_slr)\n\ny_pred_slr= slRegressor.predict(X_test_slr)\n\nprint(slRegressor)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "print('Intercept: \\n',slRegressor.intercept_)\nprint('Coefficients: \\n', slRegressor.coef_)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "two_vs_three(X_test_slr[:,0],y_test_slr,y_pred_slr,None, True)  ",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "two_d_compare(X_test_slr,y_test_slr,y_pred_slr,model_name)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "slrMetrics = model_metrics(slRegressor,y_test_slr,y_pred_slr)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "<a id=\"model_lrc\"></a>\n## 6. Build multiple linear regression model\n[Top](#top)\n\nMultiple linear regression is an extension to the simple linear regression. In this setup, the target value is dependant on more than one variable. The number of variables depends on the use case at hand. Usually a subject matter expert is involved in identifying the fields that will contribute towards better predicting the output feature.\n\ny = w0 + w1*x1 + w2*x2 + .... + wn*xn"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Since multiple linear regression assumes that output depends on more than one variable, we are assuming that it depends on all the 30 features. Data is split up into training and test sets. As an experiment, you can try to remove a few features and check if the model performs any better. "
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "X_train, X_test, y_train, y_test = train_test_split(features,label , random_state=0)\n\nprint(\"Dimensions of datasets that will be used for training : Input features\"+str(X_train.shape)+ \n      \" Output label\" + str(y_train.shape))\nprint(\"Dimensions of datasets that will be used for testing : Input features\"+str(X_test.shape)+ \n      \" Output label\" + str(y_test.shape))",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {
                "scrolled": true
            },
            "cell_type": "code",
            "source": "from sklearn.linear_model import LinearRegression\n\nmodel_name = 'Multiple Linear Regression'\n\nmlRegressor = LinearRegression()\n\nmlr_model = Pipeline(steps=[('preprocessorAll',preprocessorForAllColumns),('regressor', mlRegressor)])\n\nmlr_model.fit(X_train,y_train)\n\ny_pred_mlr= mlr_model.predict(X_test)\n\nprint(mlRegressor)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "print('Intercept: \\n',mlRegressor.intercept_)\nprint('Coefficients: \\n', mlRegressor.coef_)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "two_vs_three(X_test['YEARBUILT'],y_test,y_pred_mlr,X_test['LOTAREA'], False)  ",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "two_d_compare(X_test['YEARBUILT'],y_test,y_pred_mlr,model_name)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "mlrMetrics = model_metrics(slRegressor,y_test,y_pred_mlr)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "<a id=\"model_plr\"></a>\n## 7. Build Polynomial Linear regression model\n[Top](#top)\n\nThe prediction line generated by simple/linear regression is usually a straight line. In cases when a simple or multiple linear regression does not fit the data point accurately, we use the polynomial linear regression. The following formula is used in the back-end to generate polynomial linear regression.\n\ny = w0 + w1*x1 + w2*x21 + .... + wn*xnn"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "We are assuming that output depends on the YEARBUILT and LOTATREA. Data is split up into training and test sets. "
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "X = features.iloc[:, [0,4]].values\nX_train, X_test, y_train, y_test = train_test_split(X,label, random_state=0)\n\nprint(\"Dimensions of datasets that will be used for training : Input features\"+str(X_train.shape)+ \n      \" Output label\" + str(y_train.shape))\nprint(\"Dimensions of datasets that will be used for testing : Input features\"+str(X_test.shape)+ \n      \" Output label\" + str(y_test.shape))",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {
                "scrolled": false
            },
            "cell_type": "code",
            "source": "from sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\n\nmodel_name = 'Polynomial Linear Regression'\n\npolynomial_features= PolynomialFeatures(degree=3)\nplRegressor = LinearRegression()\n\nplr_model = Pipeline(steps=[('polyFeature',polynomial_features ),('regressor', plRegressor)])\n\nplr_model.fit(X_train,y_train)\n\ny_pred_plr= plr_model.predict(X_test)\n\nprint(plRegressor)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "print('Intercept: \\n',plRegressor.intercept_)\nprint('Coefficients: \\n', plRegressor.coef_)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "two_vs_three(X_test[:,1],y_test,y_pred_plr,X_test[:,0], False)  ",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "two_d_compare(X_test[:,1],y_test,y_pred_plr,model_name)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "plrMetrics = model_metrics(plRegressor,y_test,y_pred_plr)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "<a id=\"model_dtr\"></a>\n## 8. Build decision tree regressor\n[Top](#top)"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "\nX_train, X_test, y_train, y_test = train_test_split(features,df_pd['SALEPRICE'] , random_state=0)\n\nprint(\"Dimensions of datasets that will be used for training : Input features\"+str(X_train.shape)+ \n      \" Output label\" + str(y_train.shape))\nprint(\"Dimensions of datasets that will be used for testing : Input features\"+str(X_test.shape)+ \n      \" Output label\" + str(y_test.shape))",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from sklearn.tree import DecisionTreeRegressor\n\nmodel_name = \"Decision Tree Regressor\"\n\ndecisionTreeRegressor = DecisionTreeRegressor(random_state=0,max_features=30)\n\ndtr_model = Pipeline(steps=[('preprocessorAll',preprocessorForAllColumns),('regressor', decisionTreeRegressor)]) \n\ndtr_model.fit(X_train,y_train)\n\ny_pred_dtr = dtr_model.predict(X_test)\n\nprint(decisionTreeRegressor)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "export_graphviz(decisionTreeRegressor, out_file ='tree.dot')  \n# Use dot file to create a graph\n(graph, ) = pydot.graph_from_dot_file('tree.dot')\n# Write graph to a png file\ngraph.write_png('tree.png')",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "two_d_compare(X_test['YEARBUILT'],y_test,y_pred_dtr,model_name)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "dtrMetrics = model_metrics(decisionTreeRegressor,y_test,y_pred_dtr)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "<a id=\"model_rfr\"></a>\n## 9. Build Random Forest classification model\n[Top](#top)"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Decision tree algorithms are efficient in eliminating columns that don't add value in predicting the output and in some cases, we are even able to see how a prediction was derived by backtracking the tree. However, this algorithm doesn't perform individually when the trees are huge and are hard to interpret. Such models are oftern referred to as weak models. The model performance is however improvised by taking an average of several such decision trees derived from the subsets of the training data. This approach is called the Random Forest Regression."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from sklearn.ensemble import RandomForestRegressor\n\nmodel_name = \"Random Forest Regressor\"\n\nrandomForestRegressor = RandomForestRegressor(n_estimators=100, max_depth=15,random_state=0)\n\nrfr_model = Pipeline(steps=[('preprocessorAll',preprocessorForAllColumns),('regressor', randomForestRegressor)]) \n\nrfr_model.fit(X_train,y_train)\n\ny_pred_rfr = rfr_model.predict(X_test)\n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "two_d_compare(X_test['YEARBUILT'],y_test,y_pred_rfr,model_name)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "rfrMetrics = model_metrics(randomForestRegressor,y_test,y_pred_rfr)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "\n<a id=\"compare_classification\"></a>\n## 10. Comparative study of different regression algorithms. \n[Top](#top)\n\nIn the bar chart below, we have compared the performances of different regression algorithms with each other. \n "
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "\nn_groups = 1\nindex = np.arange(n_groups)\nbar_width = 1\nopacity = 0.8\n\n\narea = 60 \nplt.subplots(ncols=2, figsize=(12,9))\nplt.suptitle('Model performance comparison')\n\nplt.subplot(121)\nindex = np.arange(n_groups)\nbar_width = 1\nopacity = 0.8\n\nrects1 = plt.bar(index, slrMetrics[0], bar_width,\nalpha=opacity,\ncolor='g',\nlabel='Simple Linear Regression')\n\nrects2 = plt.bar(index + bar_width, mlrMetrics[0], bar_width,\nalpha=opacity,\ncolor='pink',\nlabel='Multiple Linear Regression')\n\nrects3 = plt.bar(index + bar_width*2, plrMetrics[0], bar_width,\nalpha=opacity,\ncolor='y',\nlabel='Polynomial Linear Regression')\n\nrects4 = plt.bar(index + bar_width*3, dtrMetrics[0], bar_width,\nalpha=opacity,\ncolor='b',\nlabel='Decision Tree Regression')\n\n\nrects6 = plt.bar(index + bar_width*4, rfrMetrics[0], bar_width,\nalpha=opacity,\ncolor='purple',\nlabel='Random Forest Regression')\n\nplt.xlabel('Models')\nplt.ylabel('MSE')\nplt.title('Mean Square Error comparison.')\n#ax.set_xticklabels(('', 'Simple Lin', 'Multiple Lin', 'Polynomial Lin', 'Decision Tree','Random Forest'))\n\nplt.subplot(122)\n\nrects1 = plt.bar(index, slrMetrics[1], bar_width,\nalpha=opacity,\ncolor='g',\nlabel='Simple Linear Regression')\n\nrects2 = plt.bar(index + bar_width, mlrMetrics[1], bar_width,\nalpha=opacity,\ncolor='pink',\nlabel='Multiple Linear Regression')\n\nrects3 = plt.bar(index + bar_width*2, plrMetrics[1], bar_width,\nalpha=opacity,\ncolor='y',\nlabel='Polynomial Linear Regression')\n\nrects4 = plt.bar(index + bar_width*3, dtrMetrics[1], bar_width,\nalpha=opacity,\ncolor='b',\nlabel='Decision Tree Regression')\n\n\nrects6 = plt.bar(index + bar_width*4, rfrMetrics[1], bar_width,\nalpha=opacity,\ncolor='purple',\nlabel='Random Forest Regression')\n\nplt.xlabel('Models')\nplt.ylabel('R2')\nplt.title('R2 comparison.')\nax.set_xticklabels(('', 'Simple Lin', 'Multiple Lin', 'Polynomial Lin', 'Decision Tree','Random Forest'))\n\n\n\nplt.legend()\nplt.show()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": " "
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "<p><font size=-1 color=gray>\n&copy; Copyright 2019 IBM Corp. All Rights Reserved.\n<p>\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file\nexcept in compliance with the License. You may obtain a copy of the License at\nhttps://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the\nLicense is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\nexpress or implied. See the License for the specific language governing permissions and\nlimitations under the License.\n</font></p>"
        }
    ],
    "metadata": {
        "celltoolbar": "Raw Cell Format",
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.6",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.6.9",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}